{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac52cf4d",
   "metadata": {},
   "source": [
    "# Unit Commitment Problem with Machine Learning\n",
    "In this notebook, we solve a unit commitment problem for a power system using optimization and machine learning. We use binary classification to predict the on/off status of each generator based on system conditions, aiming to accelerate the unit commitment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffc18b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0640f",
   "metadata": {},
   "source": [
    "## Step 1: Define the Unit Commitment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eae1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     N1  N2  N3  N4  N5  N6\n",
      "2019-01-01 00:00:00  10  15  20  25  30  35\n",
      "2019-01-01 01:00:00  20  25  30  35  40  45\n",
      "2019-01-01 02:00:00  30  35  40  45  50  55\n",
      "查找到的值: 25\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d10c059",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation - Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3b8fd",
   "metadata": {},
   "source": [
    "## Generate Load data based on nominal load with fluctuate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "39ae2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define constants\n",
    "nominal_load_1 = 56\n",
    "nominal_load_2 = 112\n",
    "nominal_load_3 = 112\n",
    "hours_per_year = 365 * 24\n",
    "\n",
    "# Adding sinusoidal daily cycle and random noise for variability\n",
    "time = np.arange(hours_per_year)\n",
    "daily_cycle = 0.2 * np.sin(2 * np.pi * time / 24)  # Daily sinusoidal pattern (20% amplitude)\n",
    "seasonal_cycle = 0.1 * np.sin(2 * np.pi * time / (24 * 30))  # Monthly sinusoidal pattern (10% amplitude)\n",
    "seasonal_cycle_2 = 0.1 * np.sin(3 * np.pi * time / (24 * 30)) \n",
    "random_noise = np.random.normal(0, 0.05, hours_per_year)  # Random noise\n",
    "random_noise_3 = np.random.normal(0, 0.1, hours_per_year) \n",
    "# Calculate hourly load\n",
    "hourly_load_1 = nominal_load_1 * (1 + daily_cycle + seasonal_cycle + random_noise)\n",
    "hourly_load_2 = nominal_load_2 * (1 + daily_cycle + seasonal_cycle_2 + random_noise)\n",
    "hourly_load_3 = nominal_load_3 * (1 + daily_cycle + seasonal_cycle + random_noise_3)\n",
    "\n",
    "# Create DataFrame with timestamps\n",
    "dates = pd.date_range(start=\"2019-01-01\", periods=hours_per_year, freq=\"H\")\n",
    "load_data = pd.DataFrame({\"Timestamp\": dates, \"Hourly_Load_1\": hourly_load_1, \"Hourly_Load_2\": hourly_load_2, \"Hourly_Load_3\": hourly_load_3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3255d0",
   "metadata": {},
   "source": [
    "## Generate Wind Power data from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ed48ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     N1  N2  N3         N4          N5          N6\n",
      "2019-01-01 00:00:00   0   0   0  44.960595   80.144191  116.243714\n",
      "2019-01-01 01:00:00   0   0   0  52.739302   95.669471  109.003829\n",
      "2019-01-01 02:00:00   0   0   0  49.246561   88.685836  105.381147\n",
      "2019-01-01 03:00:00   0   0   0  55.233364  100.740256  124.365467\n",
      "2019-01-01 04:00:00   0   0   0  57.948140  106.224569  140.961975\n",
      "...                  ..  ..  ..        ...         ...         ...\n",
      "2019-12-31 19:00:00   0   0   0  46.899106   89.494248  106.988068\n",
      "2019-12-31 20:00:00   0   0   0  46.717311   89.186134  111.459493\n",
      "2019-12-31 21:00:00   0   0   0  44.110204   84.029203  111.488070\n",
      "2019-12-31 22:00:00   0   0   0  45.351600   86.454089  101.468462\n",
      "2019-12-31 23:00:00   0   0   0  53.168642  101.454077  109.882804\n",
      "\n",
      "[8760 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "ninja_wind_1 = pd.read_csv(r\"D:\\46765\\MLA2\\Assignment 2\\ninja_wind_55.0582_15.1174_10.csv\")\n",
    "ninja_wind_2 = pd.read_csv(r\"D:\\46765\\MLA2\\Assignment 2\\ninja_wind_55.2402_14.7831_30.csv\")\n",
    "\n",
    "# Rename 'time' column to 'Timestamp' in both datasets for consistency\n",
    "ninja_wind_1.rename(columns={\"time\": \"Timestamp\"}, inplace=True)\n",
    "ninja_wind_2.rename(columns={\"time\": \"Timestamp\"}, inplace=True)\n",
    "\n",
    "# Convert 'Timestamp' columns to datetime format\n",
    "ninja_wind_1['Timestamp'] = pd.to_datetime(ninja_wind_1['Timestamp'], format='%Y/%m/%d %H:%M')\n",
    "ninja_wind_2['Timestamp'] = pd.to_datetime(ninja_wind_2['Timestamp'], format='%Y/%m/%d %H:%M')\n",
    "\n",
    "# Merging all data based on 'Timestamp'\n",
    "merged_data = pd.merge(load_data, ninja_wind_1, on=\"Timestamp\", how=\"outer\")\n",
    "merged_data = pd.merge(merged_data, ninja_wind_2, on=\"Timestamp\", how=\"outer\", suffixes=('_W1', '_W2'))\n",
    "\n",
    "merged_data['N1'] = 0\n",
    "merged_data['N2'] = 0\n",
    "merged_data['N3'] = 0\n",
    "merged_data['N4'] = merged_data['Hourly_Load_1'] - merged_data['power_W1']\n",
    "merged_data['N5'] = merged_data['Hourly_Load_2'] - merged_data['power_W2']\n",
    "merged_data['N6'] = merged_data['Hourly_Load_3']\n",
    "\n",
    "# Convert specified columns to dictionary format\n",
    "merged_data_dict = {\n",
    "    \"N1\": merged_data[\"N1\"].tolist(),\n",
    "    \"N2\": merged_data[\"N2\"].tolist(),\n",
    "    \"N3\": merged_data[\"N3\"].tolist(),\n",
    "    \"N4\": merged_data[\"N4\"].tolist(),\n",
    "    \"N5\": merged_data[\"N5\"].tolist(),\n",
    "    \"N6\": merged_data[\"N6\"].tolist()\n",
    "}\n",
    "\n",
    "\n",
    "D_nt_df = pd.DataFrame(merged_data_dict, index=dates)\n",
    "\n",
    "# value = D_nt_df.loc[\"2019-01-01 01:00:00\", \"N4\"]\n",
    "print(D_nt_df )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41432c2",
   "metadata": {},
   "source": [
    "## Step 1: Define the Unit Commitment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820f97b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Timestamp' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# p, p_nt\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m p \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39maddVars(G, daily_T, lb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m p_nt \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39maddVars(N, daily_T, lb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_nt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m u \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39maddVars(G, daily_T, vtype\u001b[38;5;241m=\u001b[39mGRB\u001b[38;5;241m.\u001b[39mBINARY, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \u001b[38;5;66;03m# on/off\u001b[39;00m\n",
      "File \u001b[1;32msrc\\\\gurobipy\\\\model.pxi:2914\u001b[0m, in \u001b[0;36mgurobipy.Model.addVars\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\gurobipy\\\\model.pxi:265\u001b[0m, in \u001b[0;36mgurobipy.__listify.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Timestamp' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "G = [\"G1\", \"G2\", \"G3\"] \n",
    "T = dates\n",
    "daily_time_blocks = [T[i:i+24] for i in range(0, len(T), 24)]\n",
    "\n",
    "N = [\"N1\", \"N2\", \"N3\" , \"N4\" , \"N5\", \"N6\"] \n",
    "L = [\"L1\", \"L2\", \"L3\" , \"L4\" , \"L5\", \"L6\", \"L7\"]\n",
    "\n",
    "\n",
    "# D_nt_df.set_index(['Timestamp', 'N'], inplace=True)\n",
    "B = [\n",
    "    [0, -0.68197, -0.65018, -0.48266, -0.51445, -0.63472],\n",
    "    [0, 0.146043, -0.75311, -0.22164, -0.32249, -0.70405],\n",
    "    [0, -0.31803, -0.34982, -0.51734, -0.48555, -0.36528],\n",
    "    [0, 0.17199, 0.102932, -0.26102, -0.19196, 0.069337],\n",
    "    [0, -0.14604, -0.24689, 0.221642, -0.67751, -0.29595],\n",
    "    [0, -0.14604, -0.24689, 0.221642, 0.322485, -0.29595],\n",
    "    [0, 0.146043, 0.246886, -0.22164, -0.32249, -0.70405]\n",
    "]\n",
    "max_production = [220, 100, 70]\n",
    "transmission_capacity = [200, 100, 100, 100, 100, 100, 100]\n",
    "min_down_time = [4,2,1]\n",
    "min_production = [100, 10, 10]\n",
    "min_up_time = [4,3,1]\n",
    "production_cost = [20.655, 16.7, 22.545]\n",
    "ramping_rate = [55, 50, 20]\n",
    "start_up_cost = [900,550,170]\n",
    "generator_data = pd.DataFrame({\n",
    "    'Max_Production': max_production,\n",
    "    'Min_Down_Time': min_down_time,\n",
    "    'Min_Production': min_production,\n",
    "    'Min_Up_Time': min_up_time,\n",
    "    'Production_Cost': production_cost,\n",
    "    'Ramping_Rate': ramping_rate,\n",
    "    'Start_Up_Cost': start_up_cost\n",
    "}, index=G)\n",
    "B_df = pd.DataFrame(B, index=L, columns=N)\n",
    "transmission_capacity_df = pd.DataFrame({\n",
    "    'Transmission_Capacity': transmission_capacity\n",
    "}, index=L)\n",
    "\n",
    "Load_1 = merged_data['Hourly_Load_1']\n",
    "Load_2 = merged_data['Hourly_Load_2']\n",
    "Load_3 = merged_data['Hourly_Load_3']\n",
    "P_W1 = merged_data['power_W1']\n",
    "P_W2 = merged_data['power_W2']\n",
    "\n",
    "\n",
    "u_values = {}\n",
    "p_values = {}\n",
    "daily_min_costs = []\n",
    "\n",
    "\n",
    "for day, daily_T in enumerate(daily_time_blocks):\n",
    "    # print(daily_T)\n",
    "    # D_nt_df.index = pd.to_datetime(D_nt_df.index)\n",
    "    # daily_T = pd.DatetimeIndex(daily_T)\n",
    "    # print(type(D_nt_df.index))\n",
    "    # print(type(daily_T[0])) \n",
    "    # print(daily_T[0])\n",
    "    \n",
    "    # for n in N:\n",
    "    #     for t in daily_T:\n",
    "    #         print(D_nt_df.loc[daily_T[0], n])\n",
    "   \n",
    "\n",
    "\n",
    "    model = Model(f\"DailyOptimization_Day{day+1}\")\n",
    "\n",
    "    M = 400\n",
    "\n",
    "    # p, p_nt\n",
    "    p = model.addVars(G, daily_T, lb=0,name=\"p\")\n",
    "    p_nt = model.addVars(N, daily_T, lb=0, name=\"p_nt\")\n",
    "    u = model.addVars(G, daily_T, vtype=GRB.BINARY, name=\"u\")    # on/off\n",
    "    # u = model.addVars(G, daily_T, lb=0, ub = 1, name=\"u\")\n",
    "    s = model.addVars(G, daily_T, lb=0, name=\"s\")                # start-up cost\n",
    "    epsilon = model.addVars(N, daily_T, lb=0, name=\"epsilon\") \n",
    "    delta = model.addVars(N, daily_T, lb=0, name=\"delta\")   \n",
    "\n",
    "\n",
    "\n",
    "    # Objective function\n",
    "    model.setObjective(\n",
    "        (quicksum((generator_data.loc[g,'Production_Cost'] * p[g,t] + s[g, t]) for g in G for t in daily_T) + \n",
    "        M * quicksum(epsilon[n, t] + delta[n, t] for n in N for t in daily_T)),\n",
    "        GRB.MINIMIZE\n",
    "    )\n",
    "    # Constraints for generation\n",
    "    for g in G:\n",
    "        for t in daily_T:\n",
    "            model.addConstr(generator_data.loc[g, 'Min_Production'] * u[g, t] <= p[g,t])\n",
    "            model.addConstr(p[g, t] <= generator_data.loc[g, 'Max_Production'] * u[g, t])\n",
    "\n",
    "    # balancing constraint\n",
    "    for t in daily_T:\n",
    "        model.addConstr(quicksum(p[g,t] for g in G) == quicksum(D_nt_df.loc[t, n] + epsilon[n, t] - delta[n, t] for n in N))\n",
    "\n",
    "    # Transmission limition\n",
    "    for l in L:\n",
    "        for t in daily_T:\n",
    "            model.addConstr(\n",
    "                                quicksum(B_df.loc[l, n] * (p_nt[n, t] - D_nt_df.loc[t, n] - epsilon[n, t] + delta[n, t]) for n in N) >= \n",
    "                                -transmission_capacity_df.loc[l, 'Transmission_Capacity']\n",
    "                            )\n",
    "            model.addConstr(\n",
    "                quicksum(B_df.loc[l, n] * (p_nt[n, t] - D_nt_df.loc[t, n] - epsilon[n, t] + delta[n, t]) for n in N) <= \n",
    "                transmission_capacity_df.loc[l, 'Transmission_Capacity']\n",
    "            )\n",
    "            \n",
    "    # combine p_gt with p_nt\n",
    "    for t in daily_T:\n",
    "        model.addConstr(p_nt[\"N1\", t] == p[\"G1\", t], name=f\"Link_G1_N1_t{t}\")\n",
    "        model.addConstr(p_nt[\"N2\", t] == p[\"G2\", t], name=f\"Link_G2_N2_t{t}\")\n",
    "        model.addConstr(p_nt[\"N6\", t] == p[\"G3\", t], name=f\"Link_G3_N6_t{t}\")\n",
    "        model.addConstr(p_nt[\"N3\", t] == 0, name=f\"Set_N3_Zero_t{t}\")\n",
    "        model.addConstr(p_nt[\"N4\", t] == 0, name=f\"Set_N4_Zero_t{t}\")\n",
    "        model.addConstr(p_nt[\"N5\", t] == 0, name=f\"Set_N5_Zero_t{t}\")\n",
    "\n",
    "\n",
    "    # start-up constraint\n",
    "    for g in G:\n",
    "        for t_idx, t in enumerate(daily_T):\n",
    "            if t_idx == 0:\n",
    "                # if there is data in the previous day use the final data of previous day\n",
    "                if day > 0:\n",
    "                    prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                    model.addConstr(s[g, t] >= generator_data.loc[g, 'Start_Up_Cost'] * (u[g, t] - u_values[(g, prev_day_last_t)] ))\n",
    "                # if there is no data in the previous day, no constraint\n",
    "            else:\n",
    "                # if t>0 use data in same day\n",
    "                prev_t = daily_T[t_idx - 1]\n",
    "                model.addConstr(s[g, t] >= generator_data.loc[g, 'Start_Up_Cost'] * (u[g, t] - u[g, prev_t]))\n",
    "\n",
    "    # ramping rate constraint\n",
    "    for g in G:\n",
    "        for t_idx, t in enumerate(daily_T):\n",
    "            if t_idx == 0:\n",
    "                if day > 0:\n",
    "                    prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                    model.addConstr(p[g, t] - p_values[(g, prev_day_last_t)] >= -generator_data.loc[g, 'Ramping_Rate'])\n",
    "                    model.addConstr(p[g, t] - p_values[(g, prev_day_last_t)] <= generator_data.loc[g, 'Ramping_Rate'])\n",
    "            else:\n",
    "                prev_t = daily_T[t_idx - 1]\n",
    "                print(prev_t)\n",
    "                model.addConstr(p[g, t] - p[g, prev_t] >= -generator_data.loc[g, 'Ramping_Rate'])\n",
    "                model.addConstr(p[g, t] - p[g, prev_t] <= generator_data.loc[g, 'Ramping_Rate'])\n",
    "    # min-up-time constraint\n",
    "    for g in G:\n",
    "        for t_idx, t in enumerate(daily_T):\n",
    "            max_t_in_day = daily_T[-1]\n",
    "            tau_range = pd.date_range(\n",
    "            start=t, \n",
    "            end=min(t + pd.Timedelta(hours=generator_data.loc[g, 'Min_Up_Time'] - 1), max_t_in_day), \n",
    "            freq='H'\n",
    "        )\n",
    "            # if it is the first hour of the day\n",
    "            if t_idx == 0:\n",
    "                if day > 0:\n",
    "                    prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                    for tau in tau_range:\n",
    "                        model.addConstr(\n",
    "                            -u_values[(g, prev_day_last_t)] + u[g, t] - u[g, tau] <= 0\n",
    "                        )\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                # if not the first hour of the day\n",
    "                prev_t = daily_T[t_idx - 1]\n",
    "                for tau in tau_range:\n",
    "                    model.addConstr(\n",
    "                        -u[g, prev_t] + u[g, t] - u[g, tau] <= 0\n",
    "                    )\n",
    "\n",
    "    # min-down-time constraint\n",
    "    for g in G:\n",
    "        for t_idx, t in enumerate(daily_T):\n",
    "            max_t_in_day = daily_T[-1]\n",
    "            tau_range = pd.date_range(\n",
    "            start=t, \n",
    "            end=min(t + pd.Timedelta(hours=generator_data.loc[g, 'Min_Down_Time'] - 1), max_t_in_day), \n",
    "            freq='H'\n",
    "        )\n",
    "            # if it is the first hour of the day\n",
    "            if t_idx == 0:\n",
    "                if day > 0:\n",
    "                    prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                    for tau in tau_range:\n",
    "                        model.addConstr(\n",
    "                            u_values[(g, prev_day_last_t)] - u[g, t] + u[g, tau] <= 0\n",
    "                        )\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                # if not the first hour of the day\n",
    "                prev_t = daily_T[t_idx - 1]\n",
    "                \n",
    "                for tau in tau_range:\n",
    "                    model.addConstr(\n",
    "                        u[g, prev_t] - u[g, t] + u[g, tau] <= 0\n",
    "                    )\n",
    "\n",
    "    # solve the model\n",
    "    model.optimize()\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        \n",
    "        for g in G:\n",
    "            for t in daily_T:\n",
    "                u_values[(g, t)] = u[g, t].X\n",
    "                p_values[(g, t)] = p[g, t].X\n",
    "                print(f\"Generator {g} at time {t}: Production = {p[g, t].X}, Status = {u[g, t].X}\")\n",
    "    else:\n",
    "        print(\"No optimal solution found.\")\n",
    "    # find if it is feasibility\n",
    "# # 检查是否找到可行解\n",
    "#     if model.status == GRB.OPTIMAL:\n",
    "#         print(\"Optimal solution found.\")\n",
    "        \n",
    "#         # 输出生成机的生产量和状态\n",
    "#         for g in G:\n",
    "#             for t in daily_T:\n",
    "#                 print(f\"Generator {g} at time {t}: Production = {p[g, t].X}, Status = {u[g, t].X}\")\n",
    "\n",
    "#         # 输出 p_nt 变量的值\n",
    "#         for n in N:\n",
    "#             for t in daily_T:\n",
    "#                 print(f\"p_nt[{n}, {t}] = {p_nt[n, t].X}\")\n",
    "        \n",
    "#         # 输出 s 变量的值（启动成本）\n",
    "#         for g in G:\n",
    "#             for t in daily_T:\n",
    "#                 print(f\"s[{g}, {t}] = {s[g, t].X}\")\n",
    "\n",
    "#         # 输出 epsilon 和 delta 变量的值\n",
    "#         for n in N:\n",
    "#             for t in daily_T:\n",
    "#                 print(f\"epsilon[{n}, {t}] = {epsilon[n, t].X}\")\n",
    "#                 print(f\"delta[{n}, {t}] = {delta[n, t].X}\")\n",
    "\n",
    "#         # 输出 u_values 和 p_values 的字典，方便在下一天继续使用\n",
    "#         for g in G:\n",
    "#             for t in daily_T:\n",
    "#                 u_values[(g, t)] = u[g, t].X\n",
    "#                 p_values[(g, t)] = p[g, t].X\n",
    "\n",
    "#         # 输出每日最小成本\n",
    "#         daily_min_cost = model.ObjVal\n",
    "#         daily_min_costs.append(daily_min_cost)\n",
    "#         print(f\"Minimum cost for day  {day + 1}: {daily_min_cost}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"No optimal solution found.\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3f70d",
   "metadata": {},
   "source": [
    "## Optimization For one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 00:00:00\n",
      "2019-01-01 01:00:00\n",
      "2019-01-01 02:00:00\n",
      "2019-01-01 03:00:00\n",
      "2019-01-01 04:00:00\n",
      "2019-01-01 05:00:00\n",
      "2019-01-01 06:00:00\n",
      "2019-01-01 07:00:00\n",
      "2019-01-01 08:00:00\n",
      "2019-01-01 09:00:00\n",
      "2019-01-01 10:00:00\n",
      "2019-01-01 11:00:00\n",
      "2019-01-01 12:00:00\n",
      "2019-01-01 13:00:00\n",
      "2019-01-01 14:00:00\n",
      "2019-01-01 15:00:00\n",
      "2019-01-01 16:00:00\n",
      "2019-01-01 17:00:00\n",
      "2019-01-01 18:00:00\n",
      "2019-01-01 19:00:00\n",
      "2019-01-01 20:00:00\n",
      "2019-01-01 21:00:00\n",
      "2019-01-01 22:00:00\n",
      "2019-01-01 00:00:00\n",
      "2019-01-01 01:00:00\n",
      "2019-01-01 02:00:00\n",
      "2019-01-01 03:00:00\n",
      "2019-01-01 04:00:00\n",
      "2019-01-01 05:00:00\n",
      "2019-01-01 06:00:00\n",
      "2019-01-01 07:00:00\n",
      "2019-01-01 08:00:00\n",
      "2019-01-01 09:00:00\n",
      "2019-01-01 10:00:00\n",
      "2019-01-01 11:00:00\n",
      "2019-01-01 12:00:00\n",
      "2019-01-01 13:00:00\n",
      "2019-01-01 14:00:00\n",
      "2019-01-01 15:00:00\n",
      "2019-01-01 16:00:00\n",
      "2019-01-01 17:00:00\n",
      "2019-01-01 18:00:00\n",
      "2019-01-01 19:00:00\n",
      "2019-01-01 20:00:00\n",
      "2019-01-01 21:00:00\n",
      "2019-01-01 22:00:00\n",
      "2019-01-01 00:00:00\n",
      "2019-01-01 01:00:00\n",
      "2019-01-01 02:00:00\n",
      "2019-01-01 03:00:00\n",
      "2019-01-01 04:00:00\n",
      "2019-01-01 05:00:00\n",
      "2019-01-01 06:00:00\n",
      "2019-01-01 07:00:00\n",
      "2019-01-01 08:00:00\n",
      "2019-01-01 09:00:00\n",
      "2019-01-01 10:00:00\n",
      "2019-01-01 11:00:00\n",
      "2019-01-01 12:00:00\n",
      "2019-01-01 13:00:00\n",
      "2019-01-01 14:00:00\n",
      "2019-01-01 15:00:00\n",
      "2019-01-01 16:00:00\n",
      "2019-01-01 17:00:00\n",
      "2019-01-01 18:00:00\n",
      "2019-01-01 19:00:00\n",
      "2019-01-01 20:00:00\n",
      "2019-01-01 21:00:00\n",
      "2019-01-01 22:00:00\n",
      "Gurobi Optimizer version 11.0.3 build v11.0.3rc0 (win64 - Windows 11.0 (22631.2))\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i5-12500H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 12 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 1184 rows, 648 columns and 7098 nonzeros\n",
      "Model fingerprint: 0x52e948ef\n",
      "Variable types: 576 continuous, 72 integer (72 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-02, 9e+02]\n",
      "  Objective range  [1e+00, 1e+06]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [2e-01, 4e+02]\n",
      "Found heuristic solution: objective 8.493783e+09\n",
      "Presolve removed 1167 rows and 634 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 17 rows, 14 columns, 171 nonzeros\n",
      "Found heuristic solution: objective 6.005169e+09\n",
      "Variable types: 13 continuous, 1 integer (1 binary)\n",
      "\n",
      "Root relaxation: objective 5.856458e+09, 4 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    5.856458e+09 5.8565e+09  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (4 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 3: 5.85646e+09 6.00517e+09 8.49378e+09 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 5.856457948703e+09, best bound 5.856457948703e+09, gap 0.0000%\n",
      "Generator G1 at time 2019-01-01 00:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 01:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 02:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 03:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 04:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 05:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 06:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 07:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 08:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 09:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 10:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 11:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 12:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 13:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 14:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 15:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 16:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 17:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 18:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 19:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 20:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 21:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 22:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G1 at time 2019-01-01 23:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 00:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 01:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 02:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 03:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 04:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 05:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 06:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 07:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 08:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 09:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 10:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 11:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 12:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 13:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 14:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 15:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 16:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 17:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 18:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 19:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 20:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 21:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 22:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G2 at time 2019-01-01 23:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 00:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 01:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 02:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 03:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 04:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 05:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 06:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 07:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 08:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 09:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 10:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 11:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 12:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 13:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 14:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 15:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 16:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 17:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 18:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 19:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 20:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 21:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 22:00:00: Production = 0.0, Status = 0.0\n",
      "Generator G3 at time 2019-01-01 23:00:00: Production = 20.0, Status = 1.0\n"
     ]
    }
   ],
   "source": [
    "G = [\"G1\", \"G2\", \"G3\"] \n",
    "T = dates\n",
    "daily_time_blocks = [T[i:i+24] for i in range(0, len(T), 24)]\n",
    "\n",
    "N = [\"N1\", \"N2\", \"N3\" , \"N4\" , \"N5\", \"N6\"] \n",
    "L = [\"L1\", \"L2\", \"L3\" , \"L4\" , \"L5\", \"L6\", \"L7\"]\n",
    "\n",
    "\n",
    "# D_nt_df.set_index(['Timestamp', 'N'], inplace=True)\n",
    "B = [\n",
    "    [0, -0.68197, -0.65018, -0.48266, -0.51445, -0.63472],\n",
    "    [0, 0.146043, -0.75311, -0.22164, -0.32249, -0.70405],\n",
    "    [0, -0.31803, -0.34982, -0.51734, -0.48555, -0.36528],\n",
    "    [0, 0.17199, 0.102932, -0.26102, -0.19196, 0.069337],\n",
    "    [0, -0.14604, -0.24689, 0.221642, -0.67751, -0.29595],\n",
    "    [0, -0.14604, -0.24689, 0.221642, 0.322485, -0.29595],\n",
    "    [0, 0.146043, 0.246886, -0.22164, -0.32249, -0.70405]\n",
    "]\n",
    "max_production = [220, 100, 70]\n",
    "transmission_capacity = [200, 100, 100, 100, 100, 100, 100]\n",
    "min_down_time = [4,2,1]\n",
    "min_production = [100, 10, 10]\n",
    "min_up_time = [4,3,1]\n",
    "production_cost = [20.655, 16.7, 22.545]\n",
    "ramping_rate = [55, 50, 20]\n",
    "start_up_cost = [900,550,170]\n",
    "generator_data = pd.DataFrame({\n",
    "    'Max_Production': max_production,\n",
    "    'Min_Down_Time': min_down_time,\n",
    "    'Min_Production': min_production,\n",
    "    'Min_Up_Time': min_up_time,\n",
    "    'Production_Cost': production_cost,\n",
    "    'Ramping_Rate': ramping_rate,\n",
    "    'Start_Up_Cost': start_up_cost\n",
    "}, index=G)\n",
    "B_df = pd.DataFrame(B, index=L, columns=N)\n",
    "transmission_capacity_df = pd.DataFrame({\n",
    "    'Transmission_Capacity': transmission_capacity\n",
    "}, index=L)\n",
    "\n",
    "Load_1 = merged_data['Hourly_Load_1']\n",
    "Load_2 = merged_data['Hourly_Load_2']\n",
    "Load_3 = merged_data['Hourly_Load_3']\n",
    "P_W1 = merged_data['power_W1']\n",
    "P_W2 = merged_data['power_W2']\n",
    "\n",
    "\n",
    "u_values = {}\n",
    "p_values = {}\n",
    "daily_min_costs = []\n",
    "\n",
    "day = 0  \n",
    "daily_T = daily_time_blocks[0]\n",
    "model = Model(f\"DailyOptimization_Day{day+1}\")\n",
    "\n",
    "M = 1000000\n",
    "\n",
    "# p, p_nt\n",
    "p = model.addVars(G, daily_T, lb=0,name=\"p\")\n",
    "p_nt = model.addVars(N, daily_T, lb=0, name=\"p_nt\")\n",
    "u = model.addVars(G, daily_T, vtype=GRB.BINARY, name=\"u\")    # on/off\n",
    "# u = model.addVars(G, daily_T, lb=0, ub = 1, name=\"u\")\n",
    "s = model.addVars(G, daily_T, lb=0, name=\"s\")                # start-up cost\n",
    "epsilon = model.addVars(N, daily_T, lb=0, name=\"epsilon\") \n",
    "delta = model.addVars(N, daily_T, lb=0, name=\"delta\")   \n",
    "\n",
    "\n",
    "\n",
    "# Objective function\n",
    "model.setObjective(\n",
    "    (quicksum((generator_data.loc[g,'Production_Cost'] * p[g,t] + s[g, t]) for g in G for t in daily_T) + \n",
    "    M * quicksum(epsilon[n, t] + delta[n, t] for n in N for t in daily_T)),\n",
    "    GRB.MINIMIZE\n",
    ")\n",
    "# Constraints for generation\n",
    "for g in G:\n",
    "    for t in daily_T:\n",
    "        model.addConstr(generator_data.loc[g, 'Min_Production'] * u[g, t] <= p[g,t])\n",
    "        model.addConstr(p[g, t] <= generator_data.loc[g, 'Max_Production'] * u[g, t])\n",
    "\n",
    "# balancing constraint\n",
    "for t in daily_T:\n",
    "    model.addConstr(quicksum(p[g,t] for g in G) == quicksum(D_nt_df.loc[t, n] + epsilon[n, t] - delta[n, t] for n in N))\n",
    "\n",
    "# Transmission limition\n",
    "for l in L:\n",
    "    for t in daily_T:\n",
    "        model.addConstr(\n",
    "                            quicksum(B_df.loc[l, n] * (p_nt[n, t] - D_nt_df.loc[t, n] - epsilon[n, t] + delta[n, t]) for n in N) >= \n",
    "                            -transmission_capacity_df.loc[l, 'Transmission_Capacity']\n",
    "                        )\n",
    "        model.addConstr(\n",
    "            quicksum(B_df.loc[l, n] * (p_nt[n, t] - D_nt_df.loc[t, n] - epsilon[n, t] + delta[n, t]) for n in N) <= \n",
    "            transmission_capacity_df.loc[l, 'Transmission_Capacity']\n",
    "        )\n",
    "        \n",
    "# combine p_gt with p_nt\n",
    "for t in daily_T:\n",
    "    model.addConstr(p_nt[\"N1\", t] == p[\"G1\", t], name=f\"Link_G1_N1_t{t}\")\n",
    "    model.addConstr(p_nt[\"N2\", t] == p[\"G2\", t], name=f\"Link_G2_N2_t{t}\")\n",
    "    model.addConstr(p_nt[\"N6\", t] == p[\"G3\", t], name=f\"Link_G3_N6_t{t}\")\n",
    "    model.addConstr(p_nt[\"N3\", t] == 0, name=f\"Set_N3_Zero_t{t}\")\n",
    "    model.addConstr(p_nt[\"N4\", t] == 0, name=f\"Set_N4_Zero_t{t}\")\n",
    "    model.addConstr(p_nt[\"N5\", t] == 0, name=f\"Set_N5_Zero_t{t}\")\n",
    "\n",
    "\n",
    "# start-up constraint\n",
    "for g in G:\n",
    "    for t_idx, t in enumerate(daily_T):\n",
    "        if t_idx == 0:\n",
    "            # if there is data in the previous day use the final data of previous day\n",
    "            if day > 0:\n",
    "                prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                model.addConstr(s[g, t] >= generator_data.loc[g, 'Start_Up_Cost'] * (u[g, t] - u_values[(g, prev_day_last_t)] ))\n",
    "            # if there is no data in the previous day, no constraint\n",
    "        else:\n",
    "            # if t>0 use data in same day\n",
    "            prev_t = daily_T[t_idx - 1]\n",
    "            model.addConstr(s[g, t] >= generator_data.loc[g, 'Start_Up_Cost'] * (u[g, t] - u[g, prev_t]))\n",
    "\n",
    "# ramping rate constraint\n",
    "for g in G:\n",
    "    for t_idx, t in enumerate(daily_T):\n",
    "        if t_idx == 0:\n",
    "            if day > 0:\n",
    "                prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                model.addConstr(p[g, t] - p_values[(g, prev_day_last_t)] >= -generator_data.loc[g, 'Ramping_Rate'])\n",
    "                model.addConstr(p[g, t] - p_values[(g, prev_day_last_t)] <= generator_data.loc[g, 'Ramping_Rate'])\n",
    "        else:\n",
    "            prev_t = daily_T[t_idx - 1]\n",
    "            print(prev_t)\n",
    "            model.addConstr(p[g, t] - p[g, prev_t] >= -generator_data.loc[g, 'Ramping_Rate'])\n",
    "            model.addConstr(p[g, t] - p[g, prev_t] <= generator_data.loc[g, 'Ramping_Rate'])\n",
    "# min-up-time constraint\n",
    "for g in G:\n",
    "    for t_idx, t in enumerate(daily_T):\n",
    "        max_t_in_day = daily_T[-1]\n",
    "        tau_range = pd.date_range(\n",
    "        start=t, \n",
    "        end=min(t + pd.Timedelta(hours=generator_data.loc[g, 'Min_Up_Time'] - 1), max_t_in_day), \n",
    "        freq='H'\n",
    "    )\n",
    "        # if it is the first hour of the day\n",
    "        if t_idx == 0:\n",
    "            if day > 0:\n",
    "                prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                for tau in tau_range:\n",
    "                    model.addConstr(\n",
    "                        -u_values[(g, prev_day_last_t)] + u[g, t] - u[g, tau] <= 0\n",
    "                    )\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            # if not the first hour of the day\n",
    "            prev_t = daily_T[t_idx - 1]\n",
    "            for tau in tau_range:\n",
    "                model.addConstr(\n",
    "                    -u[g, prev_t] + u[g, t] - u[g, tau] <= 0\n",
    "                )\n",
    "\n",
    "# min-down-time constraint\n",
    "for g in G:\n",
    "    for t_idx, t in enumerate(daily_T):\n",
    "        max_t_in_day = daily_T[-1]\n",
    "        tau_range = pd.date_range(\n",
    "        start=t, \n",
    "        end=min(t + pd.Timedelta(hours=generator_data.loc[g, 'Min_Down_Time'] - 1), max_t_in_day), \n",
    "        freq='H'\n",
    "    )\n",
    "        # if it is the first hour of the day\n",
    "        if t_idx == 0:\n",
    "            if day > 0:\n",
    "                prev_day_last_t = daily_time_blocks[day - 1][-1]\n",
    "                for tau in tau_range:\n",
    "                    model.addConstr(\n",
    "                        u_values[(g, prev_day_last_t)] - u[g, t] + u[g, tau] <= 0\n",
    "                    )\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            # if not the first hour of the day\n",
    "            prev_t = daily_T[t_idx - 1]\n",
    "            \n",
    "            for tau in tau_range:\n",
    "                model.addConstr(\n",
    "                    u[g, prev_t] - u[g, t] + u[g, tau] <= 0\n",
    "                )\n",
    "\n",
    "# solve the model\n",
    "model.optimize()\n",
    "if model.status == GRB.OPTIMAL:\n",
    "    \n",
    "    for g in G:\n",
    "        for t in daily_T:\n",
    "            u_values[(g, t)] = u[g, t].X\n",
    "            p_values[(g, t)] = p[g, t].X\n",
    "            print(f\"Generator {g} at time {t}: Production = {p[g, t].X}, Status = {u[g, t].X}\")\n",
    "else:\n",
    "    print(\"No optimal solution found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1290c",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f499c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "for load, wind, status in training_data:\n",
    "    for t in range(time_periods):\n",
    "        features.append([load[t], wind[t]])\n",
    "        labels.append([status[(i, t)] for i in range(len(gen_cost))])  # On/off status for each generator\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806389e",
   "metadata": {},
   "source": [
    "## Step 4: Train and Evaluate Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "for i in range(len(gen_cost)):\n",
    "    clf = LogisticRegression()\n",
    "    y_train_gen = [y[i] for y in y_train]  # Train labels for generator `i`\n",
    "    clf.fit(X_train, y_train_gen)\n",
    "    classifiers.append(clf)\n",
    "\n",
    "# Evaluate the models\n",
    "for i, clf in enumerate(classifiers):\n",
    "    y_test_gen = [y[i] for y in y_test]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'Generator {i+1} Accuracy: {accuracy_score(y_test_gen, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
